{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3r9WEuT58LWo"},"outputs":[],"source":["# import torch as th\n","# import torch.nn as nn\n","# import torch.nn.init as init\n","# import torch.nn.utils.prune as prune\n","# import torch\n","# import time\n","# #import pandas as pd\n","# import numpy as np\n","# import logging\n","# import csv\n","# from time import localtime, strftime\n","# import os\n","# import torch.nn.functional as F\n","# import torchvision\n","# import torchvision.transforms as transforms\n","# from itertools import zip_longest\n","# import matplotlib.pyplot as plt\n","# from matplotlib.ticker import PercentFormatter\n","# from torch.optim.lr_scheduler import MultiStepLR\n","# import os\n","# import random\n","# from torch.utils.tensorboard import SummaryWriter\n","\n","# seed = 42\n","# th.manual_seed(seed)\n","# random.seed(seed)\n","# np.random.seed(seed)\n","\n","# writer = SummaryWriter('results/resnet/resnet20/cifar104_1')\n","# device = 'cuda' if th.cuda.is_available() else 'cpu'\n","\n","# class Network():\n","\n","#     def weight_init(self, m):\n","#         if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","#             if self.a_type == 'relu':\n","#                 init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n","#                 init.constant_(m.bias.data, 0)\n","#             elif self.a_type == 'leaky_relu':\n","#                 init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n","#                 init.constant_(m.bias.data, 0)\n","#             elif self.a_type == 'tanh':\n","#                 g = init.calculate_gain(self.a_type)\n","#                 init.xavier_uniform_(m.weight.data, gain=g)\n","#                 init.constant_(m.bias.data, 0)\n","#             elif self.a_type == 'sigmoid':\n","#                 g = init.calculate_gain(self.a_type)\n","#                 init.xavier_uniform_(m.weight.data, gain=g)\n","#                 init.constant_(m.bias.data, 0)\n","#             else:\n","#                 raise\n","#                 return NotImplemented\n","\n","# class PruningMethod():\n","\n","#     def prune_filters(self,indices):\n","#       conv_layer=0\n","\n","\n","#       for layer_name, layer_module in self.named_modules():\n","\n","#         if(isinstance(layer_module, th.nn.Conv2d)  and layer_name!='conv1'):\n","\n","#           if(layer_name.find('conv1')!=-1):\n","#             in_channels=[i for i in range(layer_module.weight.shape[1])]\n","#             out_channels=indices[conv_layer]\n","#             layer_module.weight = th.nn.Parameter( th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])).to('cuda'))\n","\n","\n","\n","#           if(layer_name.find('conv2')!=-1):\n","#              in_channels=indices[conv_layer]\n","#              out_channels=[i for i in range(layer_module.weight.shape[0])]\n","#              layer_module.weight = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[:,in_channels])).to('cuda'))\n","#              conv_layer+=1\n","\n","\n","#           layer_module.in_channels=len(in_channels)\n","#           layer_module.out_channels=len(out_channels)\n","\n","\n","#         if (isinstance(layer_module, th.nn.BatchNorm2d) and layer_name!='bn1' and layer_name.find('bn1')!=-1):\n","#             out_channels=indices[conv_layer]\n","\n","\n","#             layer_module.weight=th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])).to('cuda'))\n","#             layer_module.bias=th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.bias.data.cpu().numpy()[out_channels])).to('cuda'))\n","\n","\n","\n","#             layer_module.running_mean= th.from_numpy(layer_module.running_mean.cpu().numpy()[out_channels]).to('cuda')\n","#             layer_module.running_var=th.from_numpy(layer_module.running_var.cpu().numpy()[out_channels]).to('cuda')\n","\n","\n","\n","#             layer_module.num_features= len(out_channels)\n","\n","#         if isinstance(layer_module, nn.Linear):\n","\n","#             break\n","\n","\n","#     def get_indices_topk(self,layer_bounds,layer_num,prune_limit,prune_value):\n","\n","#       i=layer_num\n","#       indices=prune_value[i]\n","\n","#       p=len(layer_bounds)\n","#       if (p-indices)<prune_limit:\n","#          prune_value[i]=p-prune_limit\n","#          indices=prune_value[i]\n","\n","#       k=sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[:indices]\n","#       return k\n","\n","#     def get_indices_bottomk(self,layer_bounds,i,prune_limit):\n","\n","#       k=sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[-prune_limit:]\n","#       return k\n","\n","# norm_mean, norm_var = 0.0, 1.0\n","\n","\n","# def conv3x3(in_planes, out_planes, stride=1):\n","#     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","#                      padding=1, bias=False)\n","\n","\n","# class LambdaLayer(nn.Module):\n","#     def __init__(self, lambd):\n","#         super(LambdaLayer, self).__init__()\n","#         self.lambd = lambd\n","\n","#     def forward(self, x):\n","#         return self.lambd(x)\n","\n","\n","# class ResBasicBlock(nn.Module,Network,PruningMethod):\n","#     expansion = 1\n","\n","#     def __init__(self, inplanes, planes, stride=1):\n","#         super(ResBasicBlock, self).__init__()\n","#         self.inplanes = inplanes\n","#         self.planes = planes\n","#         self.conv1 = conv3x3(inplanes, planes, stride)\n","#         self.bn1 = nn.BatchNorm2d(planes)\n","#         self.relu1 = nn.ReLU(inplace=True)\n","\n","#         self.conv2 = conv3x3(planes, planes)\n","#         self.bn2 = nn.BatchNorm2d(planes)\n","#         self.relu2 = nn.ReLU(inplace=True)\n","#         self.stride = stride\n","#         self.shortcut = nn.Sequential()\n","#         if stride != 1 or inplanes != planes:\n","#             self.shortcut = LambdaLayer(\n","#                 lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes // 4, planes-inplanes-(planes//4)), \"constant\", 0))\n","\n","#     def forward(self, x):\n","#         out = self.conv1(x)\n","#         out = self.bn1(out)\n","#         out = self.relu1(out)\n","\n","#         out = self.conv2(out)\n","#         out = self.bn2(out)\n","\n","#         out += self.shortcut(x)\n","#         out = self.relu2(out)\n","\n","#         return out\n","\n","\n","# class ResNet(nn.Module,Network,PruningMethod):\n","#     def __init__(self, block, num_layers, covcfg,num_classes=10):\n","#         super(ResNet, self).__init__()\n","#         assert (num_layers - 2) % 6 == 0, 'depth should be 6n+2'\n","#         n = (num_layers - 2) // 6\n","#         self.covcfg = covcfg\n","#         self.num_layers = num_layers\n","\n","#         self.inplanes = 16\n","#         self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","\n","#         self.bn1 = nn.BatchNorm2d(self.inplanes)\n","#         self.relu = nn.ReLU(inplace=True)\n","\n","#         self.layer1 = self._make_layer(1,block, 16, blocks=n, stride=1)\n","#         self.layer2 = self._make_layer(2,block, 32, blocks=n, stride=2)\n","#         self.layer3 = self._make_layer(3,block, 64, blocks=n, stride=2)\n","#         self.avgpool = nn.AdaptiveAvgPool2d(1)\n","\n","#         if num_layers == 110:\n","#             self.linear = nn.Linear(64 * block.expansion, num_classes)\n","#         else:\n","#             self.fc = nn.Linear(64 * block.expansion, num_classes)\n","\n","#         self.initialize()\n","#         self.layer_name_num={}\n","#         self.pruned_filters={}\n","#         self.remaining_filters={}\n","\n","#         self.remaining_filters_each_epoch=[]\n","\n","#     def initialize(self):\n","#         for m in self.modules():\n","#             if isinstance(m, nn.Conv2d):\n","#                 nn.init.kaiming_normal_(m.weight)\n","#             elif isinstance(m, nn.BatchNorm2d):\n","#                 nn.init.constant_(m.weight, 1)\n","#                 nn.init.constant_(m.bias, 0)\n","\n","#     def _make_layer(self,a, block, planes, blocks, stride):\n","#         layers = []\n","\n","#         layers.append(block(self.inplanes, planes, stride))\n","\n","#         self.inplanes = planes * block.expansion\n","#         for i in range(1, blocks):\n","#             layers.append(block(self.inplanes, planes))\n","\n","#         return nn.Sequential(*layers)\n","\n","#     def forward(self, x):\n","\n","#         x = self.conv1(x)\n","#         x = self.bn1(x)\n","#         x = self.relu(x)\n","#         x = self.layer1(x)\n","#         x = self.layer2(x)\n","#         x = self.layer3(x)\n","#         x = self.avgpool(x)\n","#         x = x.view(x.size(0), -1)\n","\n","#         if self.num_layers == 110:\n","#             x = self.linear(x)\n","#         else:\n","#             x = self.fc(x)\n","\n","#         return x\n","\n","\n","# def resnet_56():\n","#     cov_cfg = [(3 * i + 2) for i in range(9 * 3 * 2 + 1)]\n","#     return ResNet(ResBasicBlock, 56, cov_cfg)\n","# def resnet_110():\n","#     cov_cfg = [(3 * i + 2) for i in range(18 * 3 * 2 + 1)]\n","#     return ResNet(ResBasicBlock, 110, cov_cfg)\n","# def resnet_104():\n","#     cov_cfg = [(3 * i + 2) for i in range(17 * 3 * 2 + 1)]\n","#     return ResNet(ResBasicBlock, 104, cov_cfg)\n","# def resnet_32():\n","#     cov_cfg = [16, 16, 32, 64]\n","#     return ResNet(ResBasicBlock, 32, cov_cfg)\n","# def resnet_20():\n","#     cov_cfg = [(3 * i + 2) for i in range(3 * 3 * 2 + 1)]\n","#     return ResNet(ResBasicBlock, 20, cov_cfg)\n","# def resnet_34():\n","#     cov_cfg = [(3 * i + 2) for i in range(3 * 3 + 4 * 3 + 6 * 3 + 3 * 3 + 1)]\n","#     return ResNet(ResBasicBlock, 34, cov_cfg)\n","\n","# th.manual_seed(seed)\n","# th.cuda.manual_seed(seed)\n","# th.cuda.manual_seed_all(seed)\n","# th.backends.cudnn.deterministic = True\n","# N = 1\n","\n","# batch_size_tr = 128\n","# batch_size_te = 128\n","\n","# epochs = 164\n","# lr=0.1\n","# milestones_array=[82,123]\n","# lamda=0.000001\n","# weight_decay=1e-4\n","# momentum=0.9\n","\n","# prune_limits=[6]*3*3\n","# prune_value=[1]*3+[2]*3+[4]*3\n","\n","# total_layers=20\n","# total_convs=9\n","# total_blocks=3\n","\n","# # th.cuda.set_device(0)\n","# gpu = th.cuda.is_available()\n","\n","# # model=resnet_56()\n","\n","# if not gpu:\n","#     print('qqqq')\n","# else:\n","#     transform_train = transforms.Compose([\n","#     transforms.RandomCrop(32, padding=4),\n","#     transforms.RandomHorizontalFlip(),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","#     ])\n","\n","#     transform_test = transforms.Compose([\n","#         transforms.ToTensor(),\n","#         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","#     ])\n","\n","#     trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n","#     trainloader = th.utils.data.DataLoader(trainset, batch_size=batch_size_tr, shuffle=True, num_workers=2)\n","\n","#     testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n","#     testloader = th.utils.data.DataLoader(testset, batch_size=batch_size_te, shuffle=True, num_workers=2)\n","\n","# decision_count=th.ones((total_convs))\n","\n","# short=False\n","# tr_size = 50000\n","# te_size=10000\n","\n","\n","# activation = 'relu'\n","\n","# if gpu:\n","#     model=resnet_20().cuda()\n","# else:\n","#     model=resnet_20()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1732106374325,"user":{"displayName":"Akshay B","userId":"07356872784956250665"},"user_tz":-330},"id":"r23zWg72-l9Q","outputId":"bd051515-b361-458e-935a-3eac973b447b"},"outputs":[{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (layer1): Sequential(\n","    (0): ResBasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","    (1): ResBasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","    (2): ResBasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): ResBasicBlock(\n","      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): LambdaLayer()\n","    )\n","    (1): ResBasicBlock(\n","      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","    (2): ResBasicBlock(\n","      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): ResBasicBlock(\n","      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): LambdaLayer()\n","    )\n","    (1): ResBasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","    (2): ResBasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=1)\n","  (fc): Linear(in_features=64, out_features=10, bias=True)\n",")"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":634,"status":"error","timestamp":1732106374954,"user":{"displayName":"Akshay B","userId":"07356872784956250665"},"user_tz":-330},"id":"72f6YRPw-pqp","outputId":"f4d863ef-6b02-4b33-fb8e-51dc99799c38"},"outputs":[{"ename":"NameError","evalue":"name 'trainloader' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-506f20f0d0f6>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"]}],"source":["criterion = nn.CrossEntropyLoss()\n","\n","optimizer = th.optim.SGD(model.parameters(), lr=lr,momentum=0.9, weight_decay=1e-4,nesterov=True)\n","scheduler = MultiStepLR(optimizer, milestones=milestones_array, gamma=0.1)\n","\n","def evaluate(model, valid_loader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in valid_loader:\n","            images, labels = data\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    loss = total_loss / len(valid_loader)\n","    accuracy = 100 * correct / total\n","    return loss, accuracy\n","\n","\n","def custom_loss(outputs, labels, model, criterion, lambda_l1):\n","    l1_norm = 0\n","    for param in model.parameters():\n","        l1_norm += torch.sum(torch.abs(param))\n","    # Cross-entropy loss\n","    ce_loss = criterion(outputs, labels)\n","    # Total loss with L1 regularization\n","    total_loss = ce_loss - lambda_l1 * l1_norm\n","    return total_loss\n","\n","ans1='f'\n","if(ans1=='t'):\n","  checkpoint = th.load('resnet56_base.pth')\n","  model.load_state_dict(checkpoint['model'])\n","  optimizer.load_state_dict(checkpoint['optimizer'])\n","  scheduler.load_state_dict(checkpoint['scheduler'])\n","  epoch_train_acc = checkpoint['train_acc']\n","  print(epoch_train_acc,'.......')\n","  epoch_test_acc = checkpoint['test_acc']\n","  print('model loaded')\n","\n","elif(ans1=='f'):\n","\n","    best_train_acc=0\n","    best_test_acc=0\n","\n","    for n in range(1):\n","\n","        mi_iteration=0\n","        for epoch in range(epochs):\n","\n","          train_acc=[]\n","          for batch_num, (inputs, targets) in enumerate(trainloader):\n","\n","            inputs = inputs.cuda()\n","            targets = targets.cuda()\n","\n","            model.train()\n","\n","            optimizer.zero_grad()\n","            output = model(inputs)\n","            loss = criterion(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            with th.no_grad():\n","              y_hat = th.argmax(output, 1)\n","              score = th.eq(y_hat, targets).sum()\n","              train_acc.append(score.item())\n","\n","\n","          with th.no_grad():\n","            epoch_train_acc=  (sum(train_acc)*100)/tr_size\n","            test_acc=[]\n","            model.eval()\n","            for batch_nums, (inputs2, targets2) in enumerate(testloader):\n","                if(batch_nums==3 and short):\n","                    break\n","\n","                inputs2, targets2 = inputs2.cuda(), targets2.cuda()\n","                output=model(inputs2)\n","                y_hat = th.argmax(output, 1)\n","                score = th.eq(y_hat, targets2).sum()\n","                test_acc.append(score.item())\n","\n","            epoch_test_acc= (sum(test_acc)*100)/te_size\n","          writer.add_scalar('Training Loss', loss.item(), epoch)\n","          writer.add_scalar('Training Accuracy', epoch_train_acc, epoch)\n","          writer.add_scalar('Test Accuracy', epoch_test_acc, epoch)\n","\n","          print('\\n---------------Epoch number: {}'.format(epoch),\n","                  '---Train accuracy: {}'.format(epoch_train_acc),\n","                  '----Test accuracy: {}'.format(epoch_test_acc),'--------------')\n","          scheduler.step()\n","        #   print(optimizer.param_groups[0]['lr'])\n","else:\n","   print('wrong ans entered')\n","   import sys\n","   sys.exit()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uejatI-2-sRV"},"outputs":[],"source":["\n","a=[]\n","for layer_name, layer_module in model.named_modules():\n","  if(isinstance(layer_module, th.nn.Conv2d) and layer_name!='conv1' and layer_name.find('conv1')!=-1):\n","    a.append(layer_module)\n","\n","# print(\"Training accuracy of baseline model is \",epoch_train_acc,'.......')\n","# print(\"Testing accuracy of baseline model is \",epoch_test_acc,'.......')\n","\n","\n","# # ended_epoch=0\n","# best_train_acc=epoch_train_acc\n","# best_test_acc=epoch_test_acc\n","\n","# epoch=0\n","# writer.add_scalar('Baseline Accuracy/Train', epoch_train_acc, epoch)\n","# writer.add_scalar('Baseline Accuracy/Test', epoch_test_acc, epoch)\n","\n","\n","with th.no_grad():\n","\n","      #_______________________COMPUTE L1NORM____________________________________\n","    l1norm=[]\n","    l_num=0\n","    for layer_name, layer_module in model.named_modules():\n","\n","        if(isinstance(layer_module, th.nn.Conv2d) and layer_name!='conv1' and layer_name.find('conv1')!=-1):\n","            temp=[]\n","            filter_weight=layer_module.weight.clone()\n","\n","            for k in range(filter_weight.size()[0]):\n","                temp.append(float(\"{:.6f}\".format((filter_weight[k,:,:,:]).norm(1).item())))\n","\n","            l1norm.append(temp)\n","            l_num+=1\n","\n","    layer_bounds1=l1norm\n","#______Selecting__filters__to__regularize_____\n","\n","    inc_indices=[]\n","    for i in range(len(layer_bounds1)):\n","        imp_indices=model.get_indices_bottomk(layer_bounds1[i],i,prune_limits[i])\n","        inc_indices.append(imp_indices)\n","\n","\n","\n","    unimp_indices=[]\n","    dec_indices=[]\n","    for i in range(len(layer_bounds1)):\n","        temp=[]\n","        temp=model.get_indices_topk(layer_bounds1[i],i,prune_limits[i],prune_value)\n","        unimp_indices.append(temp[:])\n","        temp.extend(inc_indices[i])\n","        dec_indices.append(temp)\n","\n","    # print('selected  UNIMP indices ',unimp_indices)\n","\n","    remaining_indices=[]\n","    for i in range(total_convs):\n","      temp=[]\n","      for j in range(a[i].weight.shape[0]):\n","        if (j not in unimp_indices[i]):\n","          temp.extend([j])\n","      remaining_indices.append(temp)\n","continue_pruning=True\n","with th.no_grad():\n","\n","    if(continue_pruning==True):\n","        model.prune_filters(remaining_indices)\n","        print(model)\n","\n","ended_epoch=0\n","best_train_acc=0\n","best_test_acc=0\n","\n","\n","decision=True\n","best_test_acc= 0.0\n","prunes=1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPVN0Ftv-u02"},"outputs":[],"source":["\n","while(continue_pruning==True):\n","\n","  if(continue_pruning==True):\n","\n","\n","    if(th.sum(decision_count)==0):\n","          continue_pruning=False\n","    with th.no_grad():\n","\n","      #_______________________COMPUTE L1NORM____________________________________\n","      l1norm=[]\n","      l_num=0\n","      for layer_name, layer_module in model.named_modules():\n","\n","          if(isinstance(layer_module, th.nn.Conv2d) and layer_name!='conv1' and layer_name.find('conv1')!=-1):\n","              temp=[]\n","              filter_weight=layer_module.weight.clone()\n","\n","              for k in range(filter_weight.size()[0]):\n","                temp.append(float(\"{:.6f}\".format((filter_weight[k,:,:,:]).norm(1).item())))\n","\n","              l1norm.append(temp)\n","              l_num+=1\n","\n","      layer_bounds1=l1norm\n","\n","#______Selecting__filters__to__regularize_____\n","\n","    inc_indices=[]\n","    for i in range(len(layer_bounds1)):\n","        imp_indices=model.get_indices_bottomk(layer_bounds1[i],i,prune_limits[i])\n","        inc_indices.append(imp_indices)\n","\n","    unimp_indices=[]\n","    dec_indices=[]\n","    for i in range(len(layer_bounds1)):\n","        temp=[]\n","        temp=model.get_indices_topk(layer_bounds1[i],i,prune_limits[i],prune_value)\n","        unimp_indices.append(temp[:])\n","        temp.extend(inc_indices[i])\n","        dec_indices.append(temp)\n","\n","    # print('selected  UNIMP indices ',unimp_indices)\n","\n","    remaining_indices=[]\n","    for i in range(total_convs):\n","      temp=[]\n","      for j in range(a[i].weight.shape[0]):\n","        if (j not in unimp_indices[i]):\n","          temp.extend([j])\n","      remaining_indices.append(temp)\n","\n","    if(continue_pruning==False):\n","       lamda=0\n","#______________________Custom_Regularize the model___________________________\n","    # if(continue_pruning==True):\n","    #    optimizer = th.optim.SGD(model.parameters(), lr=optim_lr,momentum=0.9)\n","    #    scheduler = MultiStepLR(optimizer, milestones=milestones_array, gamma=0.1)\n","    optimizer = th.optim.SGD(model.parameters(), lr=lr,momentum=0.9, weight_decay=1e-4,nesterov=True)\n","    scheduler = MultiStepLR(optimizer, milestones=milestones_array, gamma=0.1)\n","\n","    epoch=0\n","    best_test_acc= 0.0\n","    for epoch in range(epochs):\n","\n","        i=0\n","        start_time = time.time()\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, labels) in enumerate(trainloader):\n","            optimizer.zero_grad()\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs)\n","            if batch_idx==0:\n","                print(\"Loss cross-entropy = \",criterion(outputs,labels))\n","                l1_reg = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n","                print(\"L1 NORM = \", l1_reg)\n","                print(\"New loss  = \",loss-(lamda*l1_reg))\n","            loss = criterion(outputs, labels)\n","            l1_reg = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n","            loss=loss-(lamda*l1_reg)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","        scheduler.step()\n","        epoch_loss = running_loss / len(trainloader.dataset)\n","        epoch_acc = 100 * correct / total\n","\n","        valid_loss, valid_acc = evaluate(model, testloader, criterion)\n","        end_time=time.time()\n","        total_time=end_time-start_time\n","\n","        print('\\nEpoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Valid Loss: {:.4f}, Valid Accuracy: {:.2f}%, Time: {:.2f}s'.format(\n","            epoch + 1, epochs, epoch_loss, epoch_acc, valid_loss, valid_acc,total_time))\n","\n","\n","    writer.add_scalar(f'Pruning_{prunes}/Train Accuracy', epoch_acc, prunes)\n","    writer.add_scalar(f'Pruning_{prunes}/Test Accuracy', valid_acc, prunes)\n","\n","    # Log the model structure (you can add it once or multiple times if the structure changes)\n","    writer.add_text(f'Pruning_{prunes}/Model Structure', str(model), prunes)\n","\n","    with th.no_grad():\n","      #_______________________COMPUTE L1NORM____________________________________\n","\n","      l1norm=[]\n","      l_num=0\n","      for layer_name, layer_module in model.named_modules():\n","\n","          if(isinstance(layer_module, th.nn.Conv2d) and layer_name!='conv1' and layer_name.find('conv1')!=-1):\n","              temp=[]\n","              filter_weight=layer_module.weight.clone()\n","              for k in range(filter_weight.size()[0]):\n","                      temp.append(float(\"{:.6f}\".format((filter_weight[k,:,:,:]).norm(1).item())))\n","              l1norm.append(temp)\n","              l_num+=1\n","\n","      layer_bounds1=l1norm\n","\n","    with th.no_grad():\n","\n","      if(continue_pruning==True):\n","        model.prune_filters(remaining_indices)\n","        print(model)\n","      else:\n","        break\n","\n","      #_________________________PRUNING_EACH_CONV_LAYER__________________________\n","      for i in range(len(layer_bounds1)):\n","          if(a[i].weight.shape[0]<= prune_limits[i]):\n","            decision_count[:]=0\n","            break\n","\n","\n","      prunes+=1\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40347,"status":"ok","timestamp":1732535264908,"user":{"displayName":"Akshay B","userId":"07356872784956250665"},"user_tz":-330},"id":"IYZmnAyxkpeu","outputId":"379391af-4fd3-4154-e102-3d88afadf997"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170M/170M [00:03<00:00, 46.7MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ../../data/cifar-10-python.tar.gz to ../../data\n","Files already downloaded and verified\n"]}],"source":["import torch as th\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.nn.utils.prune as prune\n","import torch\n","import time\n","#import pandas as pd\n","import numpy as np\n","import logging\n","import csv\n","from time import localtime, strftime\n","import os\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from itertools import zip_longest\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import PercentFormatter\n","from torch.optim.lr_scheduler import MultiStepLR\n","import os\n","import random\n","import math\n","from torch.utils.tensorboard import SummaryWriter\n","\n","seed = 42\n","th.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","writer = SummaryWriter('results/cifar-10/base')\n","device = 'cuda' if th.cuda.is_available() else 'cpu'\n","\n","class Network():\n","\n","    def weight_init(self, m):\n","        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","            if self.a_type == 'relu':\n","                init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n","                init.constant_(m.bias.data, 0)\n","            elif self.a_type == 'leaky_relu':\n","                init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n","                init.constant_(m.bias.data, 0)\n","            elif self.a_type == 'tanh':\n","                g = init.calculate_gain(self.a_type)\n","                init.xavier_uniform_(m.weight.data, gain=g)\n","                init.constant_(m.bias.data, 0)\n","            elif self.a_type == 'sigmoid':\n","                g = init.calculate_gain(self.a_type)\n","                init.xavier_uniform_(m.weight.data, gain=g)\n","                init.constant_(m.bias.data, 0)\n","            else:\n","                raise\n","                return NotImplemented\n","\n","class PruningMethod():\n","\n","    def prune_filters(self,indices):\n","      conv_layer=0\n","\n","\n","      for layer_name, layer_module in self.named_modules():\n","\n","        if(isinstance(layer_module, th.nn.Conv2d)  and layer_name!='conv1'):\n","\n","          if(layer_name.find('conv1')!=-1):\n","            in_channels=[i for i in range(layer_module.weight.shape[1])]\n","            out_channels=indices[conv_layer]\n","            layer_module.weight = th.nn.Parameter( th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])).to('cuda'))\n","\n","\n","\n","          if(layer_name.find('conv2')!=-1):\n","             in_channels=indices[conv_layer]\n","             out_channels=[i for i in range(layer_module.weight.shape[0])]\n","             layer_module.weight = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[:,in_channels])).to('cuda'))\n","             conv_layer+=1\n","\n","\n","          layer_module.in_channels=len(in_channels)\n","          layer_module.out_channels=len(out_channels)\n","\n","\n","        if (isinstance(layer_module, th.nn.BatchNorm2d) and layer_name!='bn1' and layer_name.find('bn1')!=-1):\n","            out_channels=indices[conv_layer]\n","\n","\n","            layer_module.weight=th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])).to('cuda'))\n","            layer_module.bias=th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.bias.data.cpu().numpy()[out_channels])).to('cuda'))\n","\n","\n","\n","            layer_module.running_mean= th.from_numpy(layer_module.running_mean.cpu().numpy()[out_channels]).to('cuda')\n","            layer_module.running_var=th.from_numpy(layer_module.running_var.cpu().numpy()[out_channels]).to('cuda')\n","\n","\n","\n","            layer_module.num_features= len(out_channels)\n","\n","        if isinstance(layer_module, nn.Linear):\n","\n","            break\n","\n","\n","    def get_indices_topk(self,layer_bounds,layer_num,prune_limit,prune_value):\n","\n","      i=layer_num\n","      indices=prune_value[i]\n","\n","      p=len(layer_bounds)\n","      if (p-indices)<prune_limit:\n","         prune_value[i]=p-prune_limit\n","         indices=prune_value[i]\n","\n","      k=sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[:indices]\n","      return k\n","\n","    def get_indices_bottomk(self,layer_bounds,i,prune_limit):\n","\n","      k=sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[-prune_limit:]\n","      return k\n","\n","norm_mean, norm_var = 0.0, 1.0\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","\n","class LambdaLayer(nn.Module):\n","    def __init__(self, lambd):\n","        super(LambdaLayer, self).__init__()\n","        self.lambd = lambd\n","\n","    def forward(self, x):\n","        return self.lambd(x)\n","\n","\n","class ResBasicBlock(nn.Module,Network,PruningMethod):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1):\n","        super(ResBasicBlock, self).__init__()\n","        self.inplanes = inplanes\n","        self.planes = planes\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu1 = nn.ReLU(inplace=True)\n","\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.stride = stride\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or inplanes != planes:\n","            self.shortcut = LambdaLayer(\n","                lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes // 4, planes-inplanes-(planes//4)), \"constant\", 0))\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu1(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        out += self.shortcut(x)\n","        out = self.relu2(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module,Network,PruningMethod):\n","    def __init__(self, block, num_layers, covcfg,num_classes=10):\n","        super(ResNet, self).__init__()\n","        assert (num_layers - 2) % 6 == 0, 'depth should be 6n+2'\n","        n = (num_layers - 2) // 6\n","        self.covcfg = covcfg\n","        self.num_layers = num_layers\n","\n","        self.inplanes = 16\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","\n","        self.bn1 = nn.BatchNorm2d(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.layer1 = self._make_layer(1,block, 16, blocks=n, stride=1)\n","        self.layer2 = self._make_layer(2,block, 32, blocks=n, stride=2)\n","        self.layer3 = self._make_layer(3,block, 64, blocks=n, stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","\n","        if num_layers == 110:\n","            self.linear = nn.Linear(64 * block.expansion, num_classes)\n","        else:\n","            self.fc = nn.Linear(64 * block.expansion, num_classes)\n","\n","        self.initialize()\n","        self.layer_name_num={}\n","        self.pruned_filters={}\n","        self.remaining_filters={}\n","\n","        self.remaining_filters_each_epoch=[]\n","\n","    def initialize(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def _make_layer(self,a, block, planes, blocks, stride):\n","        layers = []\n","\n","        layers.append(block(self.inplanes, planes, stride))\n","\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","\n","        if self.num_layers == 110:\n","            x = self.linear(x)\n","        else:\n","            x = self.fc(x)\n","\n","        return x\n","\n","\n","def resnet_56():\n","    cov_cfg = [(3 * i + 2) for i in range(9 * 3 * 2 + 1)]\n","    return ResNet(ResBasicBlock, 56, cov_cfg)\n","def resnet_110():\n","    cov_cfg = [(3 * i + 2) for i in range(18 * 3 * 2 + 1)]\n","    return ResNet(ResBasicBlock, 110, cov_cfg)\n","def resnet_104():\n","    cov_cfg = [(3 * i + 2) for i in range(17 * 3 * 2 + 1)]\n","    return ResNet(ResBasicBlock, 104, cov_cfg)\n","def resnet_32():\n","    cov_cfg = [16, 16, 32, 64]\n","    return ResNet(ResBasicBlock, 32, cov_cfg)\n","def resnet_20():\n","    cov_cfg = [(3 * i + 2) for i in range(3 * 3 * 2 + 1)]\n","    return ResNet(ResBasicBlock, 20, cov_cfg)\n","def resnet_34():\n","    cov_cfg = [(3 * i + 2) for i in range(3 * 3 + 4 * 3 + 6 * 3 + 3 * 3 + 1)]\n","    return ResNet(ResBasicBlock, 34, cov_cfg)\n","\n","th.manual_seed(seed)\n","th.cuda.manual_seed(seed)\n","th.cuda.manual_seed_all(seed)\n","th.backends.cudnn.deterministic = True\n","N = 1\n","\n","batch_size_tr = 128\n","batch_size_te = 128\n","\n","epochs = 164\n","lr=0.1\n","milestones_array=[82,123]\n","lamda=0.000001\n","weight_decay=1e-4\n","momentum=0.9\n","\n","prune_limits=[6]*3*3\n","prune_value=[1]*3+[2]*3+[4]*3\n","\n","total_layers=20\n","total_convs=9\n","total_blocks=3\n","\n","# th.cuda.set_device(0)\n","gpu = th.cuda.is_available()\n","\n","# model=resnet_56()\n","\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","trainset = torchvision.datasets.CIFAR10(root='../../data', train=True, download=True, transform=transform_train)\n","trainloader = th.utils.data.DataLoader(trainset, batch_size=batch_size_tr, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='../../data', train=False, download=True, transform=transform_test)\n","testloader = th.utils.data.DataLoader(testset, batch_size=batch_size_te, shuffle=True, num_workers=2)\n","\n","decision_count=th.ones((total_convs))\n","\n","short=False\n","tr_size = 50000\n","te_size=10000\n","\n","\n","activation = 'relu'\n","\n","if gpu:\n","    model=resnet_20().cuda()\n","else:\n","    model=resnet_20()\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = th.optim.SGD(model.parameters(), lr=lr,momentum=0.9, weight_decay=1e-4,nesterov=True)\n","scheduler = MultiStepLR(optimizer, milestones=milestones_array, gamma=0.1)\n","\n","def evaluate(model, valid_loader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in valid_loader:\n","            images, labels = data\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    loss = total_loss / len(valid_loader)\n","    accuracy = 100 * correct / total\n","    return loss, accuracy\n","\n","\n","def custom_loss(outputs, labels, model, criterion, lambda_l1):\n","    l1_norm = 0\n","    for param in model.parameters():\n","        l1_norm += torch.sum(torch.abs(param))\n","    ce_loss = criterion(outputs, labels)\n","    total_loss = ce_loss - lambda_l1 * l1_norm\n","    return total_loss\n","\n","def train(model, train_loader, test_loader, epochs, lr, optimizer, criterion):\n","    best_accuracy = 0.0\n","    for epoch in range(epochs):\n","        model.train()\n","        train_correct = 0\n","        train_total = 0\n","        train_loss = 0.0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            lambda_l1 = 0.01\n","            loss = custom_loss(outputs, targets, model, criterion, lambda_l1)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item() * inputs.size(0)\n","            _, predicted = outputs.max(1)\n","            train_total += targets.size(0)\n","            train_correct += predicted.eq(targets).sum().item()\n","\n","        avg_train_loss = train_loss / train_total\n","        train_accuracy = 100. * train_correct / train_total\n","\n","        test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n","\n","        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, '\n","              f'Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss:.4f}, '\n","              f'Test Accuracy: {test_accuracy:.2f}%')\n","\n","        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n","        writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n","        writer.add_scalar('Loss/Test', test_loss, epoch)\n","        writer.add_scalar('Accuracy/Test', test_accuracy, epoch)\n","\n","        scheduler.step()\n","\n","        if test_accuracy > best_accuracy:\n","            best_accuracy = test_accuracy\n","            best_model = model.state_dict()\n","\n","    model.load_state_dict(best_model)\n","    return model\n","\n","\n","\n","a=[]\n","for layer_name, layer_module in model.named_modules():\n","  if(isinstance(layer_module, th.nn.Conv2d) and layer_name!='conv1' and layer_name.find('conv1')!=-1):\n","    a.append(layer_module)\n","\n","def prune(model):\n","\n","    with th.no_grad():\n","\n","        l1norm=[]\n","        l_num=0\n","        for layer_name, layer_module in model.named_modules():\n","\n","            if(isinstance(layer_module, th.nn.Conv2d) and layer_name!='conv1' and layer_name.find('conv1')!=-1):\n","                temp=[]\n","                filter_weight=layer_module.weight.clone()\n","\n","                for k in range(filter_weight.size()[0]):\n","                    temp.append(float(\"{:.6f}\".format((filter_weight[k,:,:,:]).norm(1).item())))\n","\n","                l1norm.append(temp)\n","                l_num+=1\n","\n","        layer_bounds1=l1norm\n","\n","        inc_indices=[]\n","        for i in range(len(layer_bounds1)):\n","            imp_indices=model.get_indices_bottomk(layer_bounds1[i],i,prune_limits[i])\n","            inc_indices.append(imp_indices)\n","\n","\n","        unimp_indices=[]\n","        dec_indices=[]\n","        for i in range(len(layer_bounds1)):\n","            temp=[]\n","            temp=model.get_indices_topk(layer_bounds1[i],i,prune_limits[i],prune_value)\n","            unimp_indices.append(temp[:])\n","            temp.extend(inc_indices[i])\n","            dec_indices.append(temp)\n","\n","\n","        remaining_indices=[]\n","        for i in range(total_convs):\n","            temp=[]\n","        for j in range(a[i].weight.shape[0]):\n","            if (j not in unimp_indices[i]):\n","              temp.extend([j])\n","        remaining_indices.append(temp)\n","    continue_pruning=True\n","    with th.no_grad():\n","\n","        if(continue_pruning==True):\n","            model.prune_filters(remaining_indices)\n","\n","    return model\n","\n","def complete(model):\n","    while True:\n","        model = prune(model)\n","        print(model)\n","        train(model, trainloader, testloader, epochs, lr, optimizer, criterion)\n","    return model\n","\n","# print(model)\n","# model = complete(model)\n","\n","\n","# ended_epoch=0\n","# best_train_acc=0\n","# best_test_acc=0\n","\n","\n","# decision=True\n","# best_test_acc= 0.0\n","# prunes=1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhlm6bfWmKIg"},"outputs":[],"source":["\n","class Network():\n","    def __init__(self, model):\n","        self.model = model\n","        self.layer_outputs = {}\n","\n","    def get_layer_output(self, x, labels, layer_name):\n","        self.model.eval()\n","\n","        def hook(module, input, output):\n","          self.layer_outputs[layer_name] = output\n","\n","        hook_handle = None\n","        for name, module in self.model.named_modules():\n","            if name == layer_name:\n","                hook_handle = module.register_forward_hook(hook)\n","                break\n","\n","        with torch.no_grad():\n","          self.model(x)\n","\n","        hook_handle.remove()\n","\n","        if layer_name in self.layer_outputs:\n","            output = self.layer_outputs[layer_name]\n","        else:\n","            raise ValueError(f\"No output captured for layer: {layer_name}\")\n","\n","        labels_expanded = labels[:, None] == labels[None, :]\n","        label_matrix = torch.where(labels_expanded, 1.0, math.sqrt(2)).to(device)\n","\n","        return label_matrix, output\n","\n","    def dist_mat(self, x):\n","        if len(x.shape) == 4:\n","            x = x.view(x.shape[0], -1)\n","\n","        dist = torch.cdist(x, x, p=2)\n","        return dist\n","\n","    def diff_and_forb(self, data):\n","\n","        num_images = data.shape[0]\n","        num_filters = data.shape[1]\n","\n","        diff_matrix = data.unsqueeze(0) - data.unsqueeze(1)\n","        results = torch.norm(diff_matrix, dim=(3, 4))\n","\n","        return results\n","\n","    def compute_gaussian_kernel(self, frobenius_norm_matrices, sigma):\n","\n","        sigma = torch.tensor(sigma, device=frobenius_norm_matrices.device)\n","        gaussian_kernels = torch.exp(-frobenius_norm_matrices ** 2 / (2 * sigma ** 2))\n","        gaussian_kernels = torch.exp(-frobenius_norm_matrices ** 2 / (2 * sigma ** 2))\n","\n","        return gaussian_kernels\n","\n","    def normalize_matrix(self, matrix):\n","        assert matrix.shape[0] == matrix.shape[1], \"Input matrix must be square\"\n","\n","        matrix = matrix.cpu()\n","        normalized_matrix = np.zeros_like(matrix.numpy(), dtype=np.float64)\n","\n","        for i in range(matrix.shape[0]):\n","            for j in range(matrix.shape[1]):\n","                if i == j:\n","                    normalized_matrix[i, j] = 1.0\n","                else:\n","                    normalized_matrix[i, j] = matrix[i, j] / np.sqrt(matrix[i, i] * matrix[j, j])\n","\n","        return torch.tensor(normalized_matrix, device=matrix.device)\n","\n","    def hadamard_mult(self, data, labels):\n","        if isinstance(labels, np.ndarray):\n","            labels = torch.from_numpy(labels)\n","        if isinstance(data, np.ndarray):\n","            data = torch.from_numpy(data)\n","        broadcasted_labels = labels.unsqueeze(-1).expand_as(data)\n","        result = data * broadcasted_labels\n","        return result\n","\n","    def entropy(self, result):\n","        entropies = np.zeros(result.shape[-1])\n","        epsilon = torch.finfo(float).eps\n","\n","        for i in range(result.shape[-1]):\n","            eigenvalues = np.linalg.eigvals(result[:, :, i])\n","            eigenvalues = np.abs(eigenvalues)\n","            eigenvalues[eigenvalues == 0] = epsilon\n","\n","            entropy = -np.sum(eigenvalues * np.log2(eigenvalues))\n","\n","            entropies[i] = entropy\n","\n","        return entropies\n","\n","    def dist_mat(self, x):\n","        if len(x.shape) == 4:\n","            x = x.view(x.shape[0], -1)\n","\n","        x_squared = (x ** 2).sum(dim=1, keepdim=True)  # Shape: (N, 1)\n","        distances = torch.sqrt(x_squared - 2 * (x @ x.T) + x_squared.T)\n","        return distances\n","\n","\n","    def cal_mi(self, x, labels, conv_layers):\n","        layer_entropies = {}\n","\n","        for layer in conv_layers:\n","\n","          layer_name = layer\n","\n","          label_matrix, layer_output = self.get_layer_output(x, labels, layer_name)\n","\n","          frobenius_norm = self.diff_and_forb(layer_output)\n","\n","          gaussian_kernel = self.compute_gaussian_kernel(frobenius_norm, 2)\n","\n","          normalized_gauss = self.normalize_matrix(gaussian_kernel)\n","\n","          normalized_labels = self.normalize_matrix(label_matrix)\n","\n","          hadamard_mult = self.hadamard_mult(normalized_gauss, normalized_labels)\n","\n","          entropy = self.entropy(hadamard_mult)\n","\n","          label_matrix = self.normalize_matrix(label_matrix)\n","\n","          layer_entropies[layer_name] = entropy\n","\n","        return layer_entropies\n","\n","\n","def calculate_top_filters(base_model, train_loader, conv_layers):\n","    batch_size = 128\n","    network = Network(base_model)\n","\n","    layer_votes_batch = {}\n","    layer_aggregated_votes = {}\n","\n","    for conv in conv_layers:\n","        layer_votes_batch[conv] = []\n","\n","    print(conv_layers)\n","\n","    for batch_images, batch_labels in train_loader:\n","\n","        batch_images = batch_images.to(device)\n","        batch_labels = batch_labels.to(device)\n","        layer_votes = network.cal_mi(batch_images, batch_labels, conv_layers)\n","\n","        for layer in conv_layers:\n","            layer_votes_batch[layer].append(layer_votes[layer])\n","\n","\n","    for layer in conv_layers:\n","        layer_aggregated_votes[layer] = np.mean(layer_votes_batch[layer], axis=0)\n","\n","    return layer_aggregated_votes\n","\n","def sort(layer_aggregated_votes, conv_layers, k):\n","    layer_result = {}\n","    for layer in conv_layers:\n","        agg_votes = layer_aggregated_votes[layer]\n","        k_indices=int((agg_votes.size) * k)\n","        flat_indices = np.argsort(agg_votes.ravel())[-k_indices:]\n","        indices = np.unravel_index(flat_indices, agg_votes.shape)\n","        result = np.zeros_like(agg_votes)\n","        result[indices] = 1\n","        layer_result[layer] = result\n","\n","    return layer_result\n","\n","\n","\n","def get_important_filters(model, train_loader):\n","    conv_layers = []\n","    for name, layer in model.named_modules():\n","        if isinstance(layer, nn.Conv2d):  # Identify all convolutional layers\n","            conv_layers.append(name)\n","\n","    most_important_filters = calculate_top_filters(model, train_loader, conv_layers)\n","    sorted_filters = sort(most_important_filters, conv_layers, 0.5)\n","    return sorted_filters, conv_layers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"rrqP86QdnGec","outputId":"dce79ff3-4bf4-4b29-d61f-9102f97db19c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['conv1', 'layer1.0.conv1', 'layer1.0.conv2', 'layer1.1.conv1', 'layer1.1.conv2', 'layer1.2.conv1', 'layer1.2.conv2', 'layer2.0.conv1', 'layer2.0.conv2', 'layer2.1.conv1', 'layer2.1.conv2', 'layer2.2.conv1', 'layer2.2.conv2', 'layer3.0.conv1', 'layer3.0.conv2', 'layer3.1.conv1', 'layer3.1.conv2', 'layer3.2.conv1', 'layer3.2.conv2']\n"]}],"source":["filter_importance, conv_layers = get_important_filters(model, trainloader)\n","for name, layer in model.named_modules():\n","    if \"conv\" in name:\n","        print(name)\n","        print(layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"elapsed":6,"status":"error","timestamp":1732107266843,"user":{"displayName":"Akshay B","userId":"07356872784956250665"},"user_tz":-330},"id":"pPjthkjXol3G","outputId":"7ab626d1-58e3-4b64-b681-9dc91859d2ba"},"outputs":[{"ename":"ValueError","evalue":"'conv2' is not in list","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-71bcea0ea9c3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlayer_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mintermediate_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# output = intermediate_model(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: 'conv2' is not in list"]}],"source":["layer_idx = list(model._modules).index('conv2')\n","intermediate_model = nn.Sequential(*list(model.children())[:layer_idx+1]).to(device)\n","# output = intermediate_model(x)"]},{"cell_type":"code","source":["def train_model_resnet(model, train_loader, test_loader, epochs=250, lr=0.1, lr_drop=20, base=False, selected_filters=None, conv_layers=None):\n","    global pruning\n","    best_accuracy = 0\n","    best_model = None\n","    pruning += 1\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_drop, gamma=0.5)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss, correct, total = 0, 0, 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            if base:\n","                loss = criterion(outputs, targets)\n","            else:\n","                loss = custom_loss(outputs, targets, model, criterion, params[\"lambda_l1\"])\n","\n","            loss.backward()\n","\n","            # Gradient manipulation for selected filters\n","            for layer_name in conv_layers:\n","                # Accessing specific layer from ResNet\n","                module_hierarchy = layer_name.split('.')\n","                conv_layer = model\n","                for module_name in module_hierarchy:\n","                    conv_layer = getattr(conv_layer, module_name)\n","\n","                filters_state = list(selected_filters[layer_name])\n","                filter_mask = torch.tensor(filters_state, dtype=torch.float32).view(-1, 1, 1, 1).to(device)\n","\n","                with torch.no_grad():\n","                    conv_layer.weight.grad *= filter_mask\n","\n","            optimizer.step()\n","\n","            train_loss += loss.item() * inputs.size(0)\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        train_accuracy = 100. * correct / total\n","        avg_train_loss = train_loss / total\n","\n","        test_loss, test_accuracy = test_model(model, test_loader, criterion)\n","\n","        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n","\n","        writer.add_scalar(f'Loss/Train {pruning}Prune', train_loss, epoch)\n","        writer.add_scalar(f'Accuracy/Train {pruning}Prune', train_accuracy, epoch)\n","        writer.add_scalar(f'Loss/Test {pruning}Prune', test_loss, epoch)\n","        writer.add_scalar(f'Accuracy/Test {pruning}Prune', test_accuracy, epoch)\n","\n","        scheduler.step()\n","\n","        if test_accuracy > best_accuracy:\n","            best_accuracy = test_accuracy\n","            best_model = deepcopy(model)\n","\n","    return best_model"],"metadata":{"id":"J0ezQUwWkXI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1732107263107,"user":{"displayName":"Akshay B","userId":"07356872784956250665"},"user_tz":-330},"id":"aYqD3iSMozw-","outputId":"dc77ecb1-0a86-436a-cac8-861861e741e9"},"outputs":[{"data":{"text/plain":["Sequential(\n","  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["intermediate_model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1732108325457,"user":{"displayName":"Akshay B","userId":"07356872784956250665"},"user_tz":-330},"id":"tXe3-Vugo6IA","outputId":"69003c07-0866-4268-f359-22d9ba9b1c35"},"outputs":[{"name":"stdout","output_type":"stream","text":["conv1\n","layer1.0.conv1\n","layer1.0.conv2\n","layer1.1.conv1\n","layer1.1.conv2\n","layer1.2.conv1\n","layer1.2.conv2\n","layer2.0.conv1\n","layer2.0.conv2\n","layer2.1.conv1\n","layer2.1.conv2\n","layer2.2.conv1\n","layer2.2.conv2\n","layer3.0.conv1\n","layer3.0.conv2\n","layer3.1.conv1\n","layer3.1.conv2\n","layer3.2.conv1\n","layer3.2.conv2\n"]}],"source":["for name, layer in model.named_modules():\n","  if isinstance(layer, nn.Conv2d):\n","    print(name)\n","    # print(name)\n","    # print(type(layer))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1732109321760,"user":{"displayName":"Akshay B","userId":"07356872784956250665"},"user_tz":-330},"id":"XTCm8HkNwrn5","outputId":"270bf2c6-2209-40f2-d417-169580b20328"},"outputs":[{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (layer1): Sequential(\n","    (0): ResBasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","    (1): ResBasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","    (2): ResBasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): ResBasicBlock(\n","      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): LambdaLayer()\n","    )\n","    (1): ResBasicBlock(\n","      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","    (2): ResBasicBlock(\n","      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): ResBasicBlock(\n","      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): LambdaLayer()\n","    )\n","    (1): ResBasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","    (2): ResBasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=1)\n","  (fc): Linear(in_features=64, out_features=10, bias=True)\n",")"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["model"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLNIGqRGH/IS5BxJ89na+W"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}